---
apiVersion: v1
kind: ConfigMap
metadata:
  name: splunk-kubernetes-logging-lin
  namespace: fluentd
  labels:
    app: splunk-kubernetes-logging-lin
    version: 1.2.0
data:
  fluent.conf: |-
    @include system.conf
    @include source.containers.conf
    @include source.files.conf
    @include source.journald.conf
    @include monit.conf
    @include output.conf
  system.conf: |-
    # system wide configurations
    <system>
      log_level info
      root_dir /tmp/fluentd
    </system>
  source.containers.conf: |-
    # This configuration file for Fluentd / td-agent is used
    # to watch changes to Docker log files. The kubelet creates symlinks that
    # capture the pod name, namespace, container name & Docker container ID
    # to the docker logs for pods in the /var/log/containers directory on the host.
    # If running this fluentd configuration in a Docker container, the /var/log
    # directory should be mounted in the container.
    #
    # Reference:
    # https://github.com/kubernetes/community/blob/20d2f6f5498a5668bae2aea9dcaf4875b9c06ccb/contributors/design-proposals/node/kubelet-cri-logging.md
    #
    # Json Log Example:
    # {"log":"[info:2016-02-16T16:04:05.930-08:00] Some log text here\n","stream":"stdout","time":"2016-02-17T00:04:05.931087621Z"}
    # CRI Log Example (not supported):
    # 2016-02-17T00:04:05.931087621Z stdout [info:2016-02-16T16:04:05.930-08:00] Some log text here
    <source>
      @id containers.log
      @type tail
      @label @SPLUNK
      tag tail.containers.*
      path /var/log/containers/*.log
      pos_file /var/log/splunk-fluentd-containers.log.pos
      path_key source
      read_from_head true
      <parse>
        @type json
        time_key time
        time_type string
        time_format %Y-%m-%dT%H:%M:%S.%NZ
        localtime false
      </parse>
    </source>
  source.files.conf: "# This fluentd conf file contains sources for log files other
    than container logs."
  source.journald.conf: |-
    # This fluentd conf file contains configurations for reading logs from systemd journal.
    <source>
      @id journald-docker
      @type systemd
      @label @SPLUNK
      tag journald.kube:docker
      path "/var/log/journal"
      filters [{ "_SYSTEMD_UNIT": "docker.service" }]
      read_from_head true
      <storage>
        @type local
        persistent true
        path /var/log/splunkd-fluentd-journald-docker.pos.json
      </storage>
      <entry>
        field_map {"MESSAGE": "log", "_SYSTEMD_UNIT": "source"}
        field_map_strict true
      </entry>
    </source>
    <source>
      @id journald-kubelet
      @type systemd
      @label @SPLUNK
      tag journald.kube:kubelet
      path "/var/log/journal"
      filters [{ "_SYSTEMD_UNIT": "kubelet.service" }]
      read_from_head true
      <storage>
        @type local
        persistent true
        path /var/log/splunkd-fluentd-journald-kubelet.pos.json
      </storage>
      <entry>
        field_map {"MESSAGE": "log", "_SYSTEMD_UNIT": "source"}
        field_map_strict true
      </entry>
    </source>
  monit.conf: |-
    <source>
      @id fluentd-monitor-agent
      @type monitor_agent
      @label @SPLUNK
      tag monitor_agent
    </source>
  output.conf: |-
    <label @SPLUNK>
      # = filters for container logs =
      <filter tail.containers.var.log.containers.dns-controller*dns-controller*.log>
        @type concat
        key log
        timeout_label @SPLUNK
        stream_identity_key stream
        multiline_start_regexp /^\w[0-1]\d[0-3]\d/
        flush_interval 5s
      </filter>
      <filter tail.containers.var.log.containers.kube-dns*sidecar*.log>
        @type concat
        key log
        timeout_label @SPLUNK
        stream_identity_key stream
        multiline_start_regexp /^\w[0-1]\d[0-3]\d/
        flush_interval 5s
      </filter>
      <filter tail.containers.var.log.containers.kube-dns*dnsmasq*.log>
        @type concat
        key log
        timeout_label @SPLUNK
        stream_identity_key stream
        multiline_start_regexp /^\w[0-1]\d[0-3]\d/
        flush_interval 5s
      </filter>
      <filter tail.containers.var.log.containers.kube-apiserver*kube-apiserver*.log>
        @type concat
        key log
        timeout_label @SPLUNK
        stream_identity_key stream
        multiline_start_regexp /^\w[0-1]\d[0-3]\d/
        flush_interval 5s
      </filter>
      <filter tail.containers.var.log.containers.kube-controller-manager*kube-controller-manager*.log>
        @type concat
        key log
        timeout_label @SPLUNK
        stream_identity_key stream
        multiline_start_regexp /^\w[0-1]\d[0-3]\d/
        flush_interval 5s
      </filter>
      <filter tail.containers.var.log.containers.kube-dns-autoscaler*autoscaler*.log>
        @type concat
        key log
        timeout_label @SPLUNK
        stream_identity_key stream
        multiline_start_regexp /^\w[0-1]\d[0-3]\d/
        flush_interval 5s
      </filter>
      <filter tail.containers.var.log.containers.kube-proxy*kube-proxy*.log>
        @type concat
        key log
        timeout_label @SPLUNK
        stream_identity_key stream
        multiline_start_regexp /^\w[0-1]\d[0-3]\d/
        flush_interval 5s
      </filter>
      <filter tail.containers.var.log.containers.kube-scheduler*kube-scheduler*.log>
        @type concat
        key log
        timeout_label @SPLUNK
        stream_identity_key stream
        multiline_start_regexp /^\w[0-1]\d[0-3]\d/
        flush_interval 5s
      </filter>
      <filter tail.containers.var.log.containers.kube-dns*kubedns*.log>
        @type concat
        key log
        timeout_label @SPLUNK
        stream_identity_key stream
        multiline_start_regexp /^\w[0-1]\d[0-3]\d/
        flush_interval 5s
      </filter>
      <filter tail.containers.var.log.containers.splunk-kubernetes-logging*splunk-fluentd-k8s*.log>
        @type concat
        key log
        timeout_label @SPLUNK
        stream_identity_key stream
        multiline_start_regexp /^\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}/
        flush_interval 5s
      </filter>

      # extract index fields and sourcetype for container logs
      <filter tail.containers.**>
        @type record_transformer
        enable_ruby
        <record>
          pod ${record["source"].tap{|s| s.slice!("\\var\\log\\containers\\")}.split("_")[0]}
          namespace ${record["source"].tap{|s| s.slice!("\\var\\log\\containers\\")}.split("_")[1]}
          container_name ${record["source"].tap{|s| s.slice!("\\var\\log\\containers\\")}.split("_")[-1].split("-")[0..-2].join("-")}
          container_id ${record["source"].tap{|s| s.slice!("\\var\\log\\containers\\")}.split("_")[-1].split("-")[-1].chomp(".log")}
        </record>
      </filter>
      <filter tail.containers.**>
        @type record_transformer
        enable_ruby
        <record>
          sourcetype ${(case "#{record["container_name"]}/#{record["pod"]}"; when "dns-controller/dns-controller" then "kube:dns-controller"; when "sidecar/kube-dns" then "kube:kubedns-sidecar"; when "dnsmasq/kube-dns" then "kube:dnsmasq"; when "etcd-container/etcd-server" then "kube:etcd"; when "etcd-container/etcd-server-events" then "kube:etcd-events"; when "kube-apiserver/kube-apiserver" then "kube:kube-apiserver"; when "kube-controller-manager/kube-controller-manager" then "kube:kube-controller-manager"; when "autoscaler/kube-dns-autoscaler" then "kube:kube-dns-autoscaler"; when "kube-proxy/kube-proxy" then "kube:kube-proxy"; when "kube-scheduler/kube-scheduler" then "kube:kube-scheduler"; when "kubedns/kube-dns" then "kube:kubedns"; else nil end;) || "kube:container:#{record["container_name"]}"}
        </record>
      </filter>
      <filter tail.containers.**>
        @type record_transformer
        enable_ruby
        <record>
          sourcetype ${case "#{record["container_name"]}"; when "kube-proxy" then "kube:kube-proxy"; when "kube-flannel" then "kube:flanneld"; else "#{record["sourcetype"]}" end}
        </record>
      </filter>

      # = filters for journald logs =
      <filter journald.kube:kubelet>
        @type concat
        key log
        timeout_label @SPLUNK
        multiline_start_regexp /^\w[0-1]\d[0-3]\d/
        flush_interval 5s
      </filter>
      <filter journald.kube:kubelet journald.kube tail.containers.kube-proxy tail.containers.kube-flannel>
        @type parser
        key_name log
        <parse>
          @type regexp
          expression /^(?<severity>\w)(?<logtime>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<file>[^ \]]+)\] (?<message>.*)/
        </parse>
      </filter>

      # create source and sourcetype
      <filter journald.kube:kubelet journald.kube>
        @type record_transformer
        <record>
          source "/var/log/journal/kubelet.service"
          sourcetype "kube:kubelet"
        </record>
      </filter>
      <filter journald.kube:docker>
        @type record_transformer
        <record>
          source "/var/log/journal/docker.service"
          sourcetype "kube:docker"
        </record>
      </filter>

      # = filters for non-container log files =

      # = filters for monitor agent =
      <filter monitor_agent>
        @type record_transformer
        enable_ruby
        <record>
          source ${"namespace:#{ENV['MY_NAMESPACE']}/pod:#{ENV['MY_POD_NAME']}"}
          sourcetype ${"fluentd:monitor-agent"}
        </record>
      </filter>

      # = add cluster name to everything =
      <filter **>
        @type record_transformer
        <record>
          cluster_name "#{ENV['CLUSTER_NAME']}"
        </record>
      </filter>

      # = output =
      <match **>
        @type splunk_hec
        protocol https
        hec_host MY-SPLUNK-HOST
        hec_port 8088
        hec_token "#{ENV['SPLUNK_HEC_TOKEN']}"
        host "#{ENV['NODE_NAME']}"
        source_key source
        sourcetype_key sourcetype
        insecure_ssl true
        <fields>
          pod
          namespace
          container_name
          container_id
          cluster_name
        </fields>
        <buffer>
          @type memory
          chunk_limit_records 100000
          chunk_limit_size 200m
          flush_interval 5s
          flush_thread_count 1
          overflow_action block
          retry_max_times 3
          total_limit_size 600m
        </buffer>
        <format monitor_agent journald.kube:kubelet journald.kube tail.containers.kube-proxy tail.containers.kube-flannel>
          @type json
        </format>
        <format>
          # we just want to keep the raw logs, not the structure created by docker or journald
          @type single_value
          message_key log
          add_newline false
        </format>
      </match>
    </label>
